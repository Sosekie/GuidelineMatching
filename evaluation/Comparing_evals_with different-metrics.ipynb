{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of pipeline result files\n",
    "pipeline_files = [\n",
    "    \"grouped_result.csv\",\n",
    "    \"grouped_result.csv\",\n",
    "    \"grouped_result.csv\"\n",
    "]\n",
    "pipeline_names = [\"Pipeline 1\", \"Pipeline 2\", \"Pipeline 3\"]\n",
    "\n",
    "# Initialize metrics storage\n",
    "pipeline_metrics = []\n",
    "\n",
    "# Evaluation functions\n",
    "def precision_at_sentence_level(predicted, ground_truth):\n",
    "    if len(predicted) == 0:\n",
    "        return 0 if len(ground_truth) > 0 else 1\n",
    "    tp = len(set(predicted) & set(ground_truth))\n",
    "    return tp / len(predicted)\n",
    "\n",
    "def recall_at_sentence_level(predicted, ground_truth):\n",
    "    if len(ground_truth) == 0:\n",
    "        return 1 if len(predicted) == 0 else 0\n",
    "    tp = len(set(predicted) & set(ground_truth))\n",
    "    return tp / len(ground_truth)\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def sentence_level_accuracy(predictions, ground_truths):\n",
    "    correct = 0\n",
    "    for pred, truth in zip(predictions, ground_truths):\n",
    "        if set(pred) == set(truth):\n",
    "            correct += 1\n",
    "    return correct / len(ground_truths)\n",
    "\n",
    "# Process each pipeline\n",
    "for file, pipeline_name in zip(pipeline_files, pipeline_names):\n",
    "    # Load the pipeline results\n",
    "    pipeline_df = pd.read_csv(file)\n",
    "    grouped_result_df = pd.read_csv(\"grouped_result.csv\")  # Ground truth file\n",
    "\n",
    "    # Ensure proper formatting\n",
    "    pipeline_df[\"predicted_guidelines\"] = pipeline_df[\"predicted_guidelines\"].apply(eval)\n",
    "    grouped_result_df[\"Ausschreibungskriterium\"] = grouped_result_df[\"Ausschreibungskriterium\"].apply(eval)\n",
    "\n",
    "    # Initialize metrics\n",
    "    precisions, recalls, f1_scores, accuracies = [], [], [], []\n",
    "\n",
    "    # Calculate metrics for each sentence\n",
    "    for _, row in grouped_result_df.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        ground_truth = row[\"Ausschreibungskriterium\"]\n",
    "\n",
    "        # Match prediction\n",
    "        predicted = pipeline_df.loc[pipeline_df[\"sentence\"] == sentence, \"predicted_guidelines\"].values\n",
    "        predicted = predicted[0] if len(predicted) > 0 else []\n",
    "\n",
    "        precision = precision_at_sentence_level(predicted, ground_truth)\n",
    "        recall = recall_at_sentence_level(predicted, ground_truth)\n",
    "        f1 = f1_score(precision, recall)\n",
    "        accuracy = 1 if set(predicted) == set(ground_truth) else 0\n",
    "\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1 = f1_score(avg_precision, avg_recall)\n",
    "    avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "\n",
    "    # Store pipeline metrics\n",
    "    pipeline_metrics.append({\n",
    "        \"Pipeline\": pipeline_name,\n",
    "        \"Precision\": avg_precision,\n",
    "        \"Recall\": avg_recall,\n",
    "        \"F1-Score\": avg_f1,\n",
    "        \"Accuracy\": avg_accuracy\n",
    "    })\n",
    "\n",
    "# Convert metrics to a DataFrame\n",
    "metrics_df = pd.DataFrame(pipeline_metrics)\n",
    "\n",
    "# Visualization\n",
    "metrics_long = metrics_df.melt(id_vars=\"Pipeline\", var_name=\"Metric\", value_name=\"Score\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=metrics_long, x=\"Metric\", y=\"Score\", hue=\"Pipeline\")\n",
    "plt.title(\"Comparison of Metrics Across Pipelines\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(title=\"Pipeline\")\n",
    "plt.show()\n",
    "\n",
    "metrics = [\"Precision\", \"Recall\", \"F1-Score\", \"Accuracy\"]\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(data=metrics_df, x=\"Pipeline\", y=metric, palette=\"viridis\")\n",
    "    plt.title(f\"{metric} Comparison Across Pipelines\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(metric)\n",
    "    plt.xlabel(\"Pipeline\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
